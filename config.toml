[global]
dataset = "mastero"
random_slice_seed = 123
val_ratio = 0.1
train_val_split_seed = 666

[model.transformer]
lr = 1e-3 # TODO: we can use a learning rate scheduler
seq_len = 64
batch_size = 64
n_notes = 128
embed_dim = 128
hidden_dim = 1024
clip_grad = 5.0
num_encoder_layers = 3
num_decoder_layers = 3
num_heads = 8
src_vocab_size = 2
tgt_vocab_size = 128

[model.lstm]
lr = 1e-3 # TODO: we can use a learning rate scheduler
seq_len = 64
batch_size = 64
n_notes = 128
embed_dim = 32
hidden_dim = 256
clip_grad = 5.0

[model.lstm_attn]
lr = 1e-3
seq_len = 64
batch_size = 64
n_notes = 128
hidden_dim = 1024
dropout_p = 0.5
clip_grad = 5.0

[model.vanilla_rnn]
lr = 1e-3 
seq_len = 64
batch_size = 64
n_notes = 128
embed_dim = 32
hidden_dim = 256
clip_grad = 5.0

[model.attention_rnn]
lr = 1e-3 
seq_len = 64
batch_size = 64
n_notes = 128
embed_dim = 32
encode_hidden_dim = 512
decode_hidden_dim = 1024
clip_grad = 5.0

[model.cnn]
lr = 1e-3
embed_dim = 32

[sampling.default]
strategy = "stochastic"
top_p = 0.9
top_k = 4
repeat_decay = 0.6
temperature = 1.5
hint = []

[sampling.beta]
strategy = "stochastic"
top_p = 0.9
top_k = 4
repeat_decay = 0.6
temperature = 1.2
hint = ["1", "2", "3", "4"]

[sampling.beam]
strategy = "beam"
repeat_decay = 0.5
hint = ["1", "2", "3", "4"]
num_beams = 5
beam_prob = 0.5
temperature = 1