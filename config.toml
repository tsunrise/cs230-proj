[global]
dataset = "mastero"
random_slice_seed = 123
val_ratio = 0.1
train_val_split_seed = 666
initial_note = 60

[model.transformer]
lr = 1e-3 # TODO: we can use a learning rate scheduler
seq_len = 128
batch_size = 64
n_notes = 128
embed_dim = 32
hidden_dim = 256
clip_grad = 5.0
num_encoder_layers = 3
num_decoder_layers = 3
num_heads = 8
src_vocab_size = 2
tgt_vocab_size = 128

[model.lstm]
lr = 1e-3 # TODO: we can use a learning rate scheduler
seq_len = 64
batch_size = 64
n_notes = 128
embed_dim = 32
hidden_dim = 256
clip_grad = 5.0

[model.vanilla_rnn]
lr = 1e-3 
seq_len = 64
batch_size = 64
n_notes = 128
embed_dim = 32
hidden_dim = 256
clip_grad = 5.0

[model.attention_rnn]
lr = 1e-3 
seq_len = 64
batch_size = 64
n_notes = 128
embed_dim = 32
hidden_dim = 256
clip_grad = 5.0

[model.bi_lstm]
lr = 1e-3 
seq_len = 64
batch_size = 64
n_notes = 128
embed_dim = 32
hidden_dim = 256
clip_grad = 5.0

[model.cnn]
lr = 1e-3
embed_dim = 32

[sampling.default]
strategy = "greedy"
init_note = 65
top_p = 0.8
top_k = 3
repeat_decay = 0.8
temperature = 1.2
